---
title: \vspace{-0.75cm} Final Project Report \vspace{-0.5cm} 
subtitle: |
  | STAT 639-700
  | Samuel Burge (UIN: 331008088)
  | Chad Austgen (UIN: 727007397)
  | Skylar Liu (UIN: 727007397)
  | April 19, 2022
output: pdf_document
header-includes:
    - \usepackage{setspace}\doublespacing
    - \vspace{0cm}
bibliography: bibliography.bib
fontsize: 12pt
geometry: margin=1in
---

```{r setup, include=FALSE}
# Set working directory
setwd("C:\\Users\\SamBu\\Documents\\GitHub\\data-mining-project")
load('cv_final_results.RData')

# Import the necessary packages
require(tidyverse)
require(kableExtra)
require(MASS)
require(class)
require(glmnet)
require(ROCR)
require(e1071)
require(naivebayes)
require(tree)
require(randomForest)
require(xgboost)
require(gbm)

# Set table options for pdf output
options(knitr.table.format = "latex")
```

# Classification Task

For the classification task we used the Naive Bayes classifier, regularized logistic regression (specifically ridge, lasso and elastic net regularization), gradient boosted trees, random forests and support vector machines with radial and polynomial kernels. For model selection and assessment, we utilized 10-fold cross-validation and, when needed, another nested cross-validation to select tuning hyper-parameter using a grid search. Nested cross-validation is a well-known approach to handling both tuning and assessment of models with hyper-parameters, and we opted to employ this approach to avoid potential over-fitting and bias introduced when the same data used to validate the models are also used in parameter tuning [@cawley].

Before the analysis we created plot matrices and verified across all predictors that $X$ and $Y$ were not well separated, suggesting logistic regression might be a good model. However, we were not able to fit a logistic regression model outright since $p > n$. We utilized regularization methods to reduce the features in our analysis using lasso, ridge, and elastic net regularization methods. We also opted to use a different boosting algorithm for the classification trees, XGBoost, compared to the algorithms found in R's gbm package. This was primarily due to the computational resources necessary to perform nested cross-validations, as XGBoost is known for it's impressive predictive performance and computational efficiency [@xgboost]. 

The results of our analysis are depicted in the table below. Overall, the tree-based methods had the best performance of all the classifiers and the boosted classification trees had the best generalization performance with a CV error rate of `r round(boost.cv.error, 2)*100`%. After re-fitting the final model using the same grid search procedure, our model's tuning parameters were an 80% subsample ratio the training instances, a learning rate (shrinkage penalty) of 0.001 ,and a max interaction depth for each tree of 4. These tuning parameters were selected primarily to help us avoid over-fitting the model given the large amount of flexibility these algorithms can provide if left unchecked, although the interaction depth was deeper than we expected, which could suggest that there are some interactions between the variables in the data.

```{r fold-errors-plot, echo = FALSE, out.height='40%', include = FALSE}
foldErrors <- data.frame(boost.fold.errors,lasso.fold.errors,net.fold.errors,
                       ridge.fold.errors,naive.fold.errors,radialSVM.fold.errors,polySVM.fold.errors)

foldErrors$RForest <- randforest_error_rate
colnames(foldErrors) <- c('Boosted Trees','Lasso','E.Net','Ridge',
                          'N.Bayes','Radial SVM','Poly SVM','R. Forest')

boxplot(foldErrors, xlab='Model',
        ylab='CV Fold Misclassification Rates', main='Model Comparison')

```

```{r results-table, echo = FALSE, out.height='40%', fig.align ='center'}
errors.matrix <- round(cbind(train.errors, cv.errors), 2)
colnames(errors.matrix) <- c('Train Error', 'Estimated Test Error')

kableExtra::kable(round(cbind(train.errors, cv.errors),4), booktabs = T, align = c('c', 'c'),
      col.names = c('Training Error', 'Est. Test Error'),
      caption = "Results from 10-fold cross-validation to assess model generalization performance.") %>%
  kable_styling(latex_options = 'hold_position') %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2:7, width = "2in", )
```

# Clustering Task

```{r clustering-setup, echo=FALSE, message=FALSE, include=FALSE}
# Load the necessary packages for the clustering task
require(factoextra)
require(gridExtra)
require(cluster)
require(NbClust)
require(dbscan)
require(ggdendro)

# Set working directory and import the data file
load("cluster_data.RData")
load('clusterCharts.RData')
```

For the clustering task, we initially looked at performing principal components analysis (PCA) to reduce the number of dimensions in the data set. The two scree plots below show that the number of principal components necessary to capture at least 90% of the variation in the data set was about 187, which did not seem beneficial enough to consider for the analysis. Therefore, we decided to retain all the original features in the data set.

```{r pca, echo=FALSE, fig.height=2.5, fig.width=7, include = FALSE, fig.align ='center', fig.cap='Scree plots from PCA analysis.'}
# Calculate the principal components
y.pca <- prcomp(y, scale = TRUE, center = TRUE)

# Compute the proportion of variance explained (PVE)
pr.var <- (y.pca$sdev)^2
pve <- pr.var / sum(pr.var)

# See at which principal component(s) we have 90%+ of the cum. variance explained
cume.pve <- cumsum(pve)

# Plot the two plots side-by-side
par(mfrow=c(1,2))

# Scree plot
plot(pve, ylim = c(0,1), type = 'l', col = 'blue',
     xlab = "Principal Component",
     ylab = "PVE")

# Cumulative scree plot
plot(cumsum(pve), type = 'l', col = 'blue',
     xlab="Principal Component",
     ylab=NA)
```

We opted to use k-means and hierarchical clustering, in part because the high-dimensional data is intractable with DBSCAN and due to computing restraints. Since the given data does not have any contextual basis for selecting the number of clusters $K$, we used several widely used approaches include the elbow method, the silhouette method [@rousseeuw], and more recently the use of the gap statistic [@tibshirani]. We decided on $K = 7$.
\ 

```{r gap-plots, fig.width = 7, fig.height = 4, echo = FALSE, error = FALSE, message = FALSE}
# Specify how the plots will be output
grid.arrange(kmean.sil + ylab("Avg. Silhouette") + ggtitle("") + xlab("") +
               ggtitle("Opt. Number of Clusters") + scale_x_discrete(breaks = ""),
             
             hclust.sil + ylab("") + ggtitle("") + xlab("") +
               ggtitle("Opt. Number of Clusters") + scale_x_discrete() +  scale_x_discrete(breaks = ""),
             
             kmean.gap + ylab("Gap statistic") + ggtitle("") +
               scale_x_discrete(breaks = c(1,3,5,7,9,11,13, 15)), 
             
             hclust.gap + ylab("") + ggtitle("") +
               scale_x_discrete(breaks = c(1,3,5,7,9,11,13, 15)),
             
             nrow = 3, ncol = 2)
```

The graphs suggested possibly using 14, but we ultimately decided on 7 which is easier to interpret. The results of the K-means and hierarchical clustering methods are depicted in the bar graphs below. K-Means grouped the data into relatively equal sized clusters, with cluster 5 having slightly more observations. Hierarchical clustering resulted in the majority of our data falling into cluster 2, which was consistent with the dendrogram for complete linkage hierarchical cluster. Because of the difficulty visualzing data in high dimensions in the data, we plotted bar graphs of the clusters from both methods.
```{r final_cluster_barplots, echo = FALSE, fig.height=4, fig.width=7, fig.align='center', fig.cap='Count of observations in each cluster for both clustering methods.'}
par(mfrow=c(1,2))

barplot(table(km7$cluster), 
          main='Distribution of K-Means Clusters', ylab="Frequency", xlab="Cluster")

barplot(table(hcut7),
          main='Distribution of Hierarchical Clusters', ylab="Frequency", xlab="Cluster")
```
\footnotesize

