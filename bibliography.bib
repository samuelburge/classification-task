@article{rousseeuw,
    author    = "Peter J. Rousseeuw",
    title     = "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis",
    journal   = "Journal of Computational and Applied Mathematics",
    volume   = "Volume 20",
    number   = "https://doi.org/10.1016/0377-0427(87)90125-7",
    pages    = "53-65",
    year      = "1987",
    month    = "",
    note     = "ISSN 0377-0427",
}

@article{cawley,
  author  = {Gavin C. Cawley and Nicola L. C. Talbot},
  title   = {On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {70},
  pages   = {2079-2107},
  url     = {http://jmlr.org/papers/v11/cawley10a.html}
}

@article{hastie,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/3647580},
 abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p ≫ n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
 author = {Hui Zou and Trevor Hastie},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {2},
 pages = {301--320},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regularization and Variable Selection via the Elastic Net},
 urldate = {2022-04-19},
 volume = {67},
 year = {2005}
}

@article{tibshirani,
author = {Tibshirani, Robert and Walther, Guenther and Hastie, Trevor},
title = {Estimating the number of clusters in a data set via the gap statistic},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {63},
number = {2},
pages = {411-423},
keywords = {Clustering, Groups, Hierarchy, Uniform distribution},
doi = {https://doi.org/10.1111/1467-9868.00293},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00293},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00293},
abstract = {We propose a method (the ‘gap statistic’) for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.},
year = {2001}
}

@inproceedings{xgboost,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939785},
doi = {10.1145/2939672.2939785},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

  